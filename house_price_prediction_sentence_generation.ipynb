{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90dd9b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import zipfile\n",
    "import random\n",
    "import time\n",
    "import csv\n",
    "import datetime\n",
    "from itertools import compress\n",
    "from collections import Counter, defaultdict\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "from transformers import AutoTokenizer, AutoConfig, AutoModelForPreTraining, \\\n",
    "                         AdamW, get_linear_schedule_with_warmup, \\\n",
    "                         TrainingArguments, BeamScorer, Trainer\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, random_split, DataLoader, \\\n",
    "                             RandomSampler, SequentialSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d1db1227",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEBUG           = False\n",
    "\n",
    "INPUT_DIR       = 'articles'\n",
    "\n",
    "USE_APEX        = True\n",
    "APEX_OPT_LEVEL  = 'O1'\n",
    "\n",
    "MODEL           = 'gpt2' #{gpt2, gpt2-medium, gpt2-large, gpt2-xl}\n",
    "\n",
    "UNFREEZE_LAST_N = 6 #The last N layers to unfreeze for training\n",
    "\n",
    "SPECIAL_TOKENS  = { \"bos_token\": \"<|BOS|>\",\n",
    "                    \"eos_token\": \"<|EOS|>\",\n",
    "                    \"unk_token\": \"<|UNK|>\",                    \n",
    "                    \"pad_token\": \"<|PAD|>\",\n",
    "                    \"sep_token\": \"<|SEP|>\"}\n",
    "                    \n",
    "MAXLEN          = 384  #{768, 1024, 1280, 1600}\n",
    "\n",
    "TRAIN_SIZE      = 0.8\n",
    "\n",
    "if USE_APEX:\n",
    "    TRAIN_BATCHSIZE = 4\n",
    "    BATCH_UPDATE    = 16\n",
    "else:\n",
    "    TRAIN_BATCHSIZE = 2\n",
    "    BATCH_UPDATE    = 32\n",
    "\n",
    "EPOCHS          = 4\n",
    "LR              = 5e-4\n",
    "EPS             = 1e-8\n",
    "WARMUP_STEPS    = 1e2\n",
    "\n",
    "SEED            = 2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2dcfa34d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokenier(special_tokens=None):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL) #GPT2Tokenizer\n",
    "\n",
    "    if special_tokens:\n",
    "        tokenizer.add_special_tokens(special_tokens)\n",
    "        print(\"Special tokens added\")\n",
    "    return tokenizer\n",
    "\n",
    "def get_model(tokenizer, special_tokens=None, load_model_path=None):\n",
    "\n",
    "    #GPT2LMHeadModel\n",
    "    if special_tokens:\n",
    "        config = AutoConfig.from_pretrained(MODEL, \n",
    "                                            bos_token_id=tokenizer.bos_token_id,\n",
    "                                            eos_token_id=tokenizer.eos_token_id,\n",
    "                                            sep_token_id=tokenizer.sep_token_id,\n",
    "                                            pad_token_id=tokenizer.pad_token_id,\n",
    "                                            output_hidden_states=False)\n",
    "    else: \n",
    "        config = AutoConfig.from_pretrained(MODEL,                                     \n",
    "                                            pad_token_id=tokenizer.eos_token_id,\n",
    "                                            output_hidden_states=False)    \n",
    "\n",
    "    #----------------------------------------------------------------#\n",
    "    model = AutoModelForPreTraining.from_pretrained(MODEL, config=config)\n",
    "\n",
    "    if special_tokens:\n",
    "        #Special tokens added, model needs to be resized accordingly\n",
    "        model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "    if load_model_path:\n",
    "        model.load_state_dict(torch.load(load_model_path))\n",
    "\n",
    "    model.cuda()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "63514593",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Special tokens added\n"
     ]
    }
   ],
   "source": [
    "tokenizer = get_tokenier(special_tokens=SPECIAL_TOKENS)\n",
    "model = get_model(tokenizer, \n",
    "                  special_tokens=SPECIAL_TOKENS,\n",
    "                #   load_model_path='pytorch_model.bin'\n",
    "                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "251ce2cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "seed_everything(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "440f00cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_data = pd.read_csv('all_after_preprocessingStopwords.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "751b3801",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'beautiful bright spacious home heart lake community right highway 410 approximately 2450 space 670 basement 4 4 separate entrance garage basement full potential great rental income hardwood pot light california shutter throughout freshly painted open concept floor plan eat stainless steel appliance minute trinity common mall park heart lake conservation much stainless steel stove fridge dishwasher washer dryer water heater rental existing light hardwood throughout concrete pad leading front backyard fit upto 4 car 1 largest semi detached model'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numerical_data['description'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fc5356d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "from transformers import BertTokenizer,BertModel,get_linear_schedule_with_warmup, RobertaTokenizer, RobertaModel\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "def Data_preprocessing(df):\n",
    "    scaler = MinMaxScaler()\n",
    "    \n",
    "    # Numerical Feature\n",
    "    numerical_features = [\"bedroom\",\"bedroomAboveGrade\",\"bedroomBelowGrade\",\"bathroom\", \"bathroomTotal\",\"bathroomPartial\", \n",
    "                        \"totalParkingSpaces\", \"storeys\", \"maintenanceFees\",  'landSize']\n",
    "    \n",
    "    X_num = df[numerical_features]\n",
    "    X_num = scaler.fit_transform(X_num)\n",
    "    X_num = pd.DataFrame(X_num)\n",
    "    df[\"longitude\"] = df[\"longitude\"] *0.01\n",
    "    df[\"latitude\"] = df[\"latitude\"] *0.01\n",
    "    df[numerical_features] = X_num\n",
    "          \n",
    "    # Boolean Feature\n",
    "    boolean_features=['parkingAttachedGarage',\n",
    "       'parkingUnderground', 'parkingInsideEntry', 'parkingSurfaced',\n",
    "       'parkingOversize', 'parkingGravel', 'parkingGarage', 'parkingShared',\n",
    "       'parkingDetachedGarage', 'parkingCarport', 'parkingInterlocked',\n",
    "       'parkingVisitorParking','amenityClubhouse', 'amenityCarWash', 'amenityMusicRoom',\n",
    "       'amenityStorageLocker', 'amenitySauna', 'amenityPartyRoom',\n",
    "       'amenityRecreationCentre', 'amenityGuestSuite', 'amenityFurnished',\n",
    "       'amenityLaundryFacility', 'amenityExerciseCentre',\n",
    "       'amenityLaundryInSuite', 'amenitySecurity', 'amenityWhirlpool',\n",
    "       'efinishWood', 'efinishBrick', 'efinishHardboard', 'efinishWoodsiding',\n",
    "       'efinishLog', 'efinishMetal', 'efinishSteel', 'efinishStone',\n",
    "       'efinishWoodshingles', 'efinishStucco', 'efinishSiding',\n",
    "       'efinishConcrete', 'efinishShingles', 'efinishAluminumsiding',\n",
    "       'efinishCedarshingles', 'efinishVinyl', 'efinishVinylsiding',\n",
    "       'featurePetNotAllowed', 'AirportNearby',\n",
    "       'GolfNearby', 'MarinaNearby', 'ShoppingNearby', 'WaterNearby',\n",
    "       'WorshipPlaceNearby', 'RecreationNearby', 'PlaygroundNearby',\n",
    "       'PublicTransitNearby', 'ParkNearby', 'SchoolsNearby', 'HospitalNearby',\n",
    "       'HighwayNearby', 'SkiAreaNearby']\n",
    "    \n",
    "    # Category Feature\n",
    "    cate_features = ['city', 'typeBuilding', 'title', 'styleAttach', \n",
    "                   'cooling',  'basementType', 'basementFinish',\n",
    "                   'heatingType1', 'heatingType2', 'heatingEnergy1', 'heatingEnergy2', \n",
    "                   'featureLotSlope', 'featureDriveway', 'featureLotPositionType',\n",
    "       'featureOutdoorAreaType', 'featureOutdoorLandscape',\n",
    "       'featureAdditionalFacility']\n",
    "    \n",
    "    X_category=df[cate_features]\n",
    "    for col in cate_features:\n",
    "        X_category[col] = X_category[col].astype('category')\n",
    "        X_category[col] = X_category[col].cat.codes\n",
    "    df[cate_features] = X_category\n",
    "    \n",
    "    # Label Price\n",
    "    price_range = []\n",
    "    \n",
    "    for price in df[\"price\"]:\n",
    "        if price < 5e5:\n",
    "            price_range.append('tinny size apartment with shared public infrastructure')\n",
    "        elif 5e5<=price < 15e5:\n",
    "            price_range.append('bachelor apartment with limited private infrastructure')\n",
    "        elif 15e5<=price < 25e5:\n",
    "            price_range.append('family applicable apartments with standard community service and private infrastructure')\n",
    "        elif 25e5<=price < 35e5:\n",
    "            price_range.append('large and well furnished apartment at prosperous district')\n",
    "        elif 35e5<=price < 80e5:\n",
    "            price_range.append('superior apartment with upscale customization services at commerce center area')\n",
    "        else:\n",
    "            price_range.append('luxury villa with large private yard')\n",
    "            \n",
    "#     df = df.reset_index(drop=True)\n",
    "    df['price_range'] = price_range\n",
    "    df = df.dropna()\n",
    "    return df, boolean_features, cate_features, numerical_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c99d660f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-8-8e71f3a6c12e>:50: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_category[col] = X_category[col].astype('category')\n",
      "<ipython-input-8-8e71f3a6c12e>:51: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_category[col] = X_category[col].cat.codes\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "preprocessed_data, boolean_features, cate_features, numerical_features = Data_preprocessing(numerical_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "413fb9d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test = train_test_split(preprocessed_data, test_size=0.1, random_state=13) \n",
    "X_train = X_train.reset_index(drop=True)\n",
    "X_test = X_test.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0e2c4cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generate_dataset(Dataset):\n",
    "    def __init__(self, data_df, tokenizer):\n",
    "        self.MAX_SEQ_LEN = 128\n",
    "        self.data = data_df\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx): \n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        description = self.data.iloc[idx]['description']\n",
    "        label_range =  self.data.iloc[idx]['price_range']\n",
    "        description_final = SPECIAL_TOKENS['bos_token'] + label_range + SPECIAL_TOKENS['sep_token'] + description + SPECIAL_TOKENS['eos_token']\n",
    "        encodings_dict = self.tokenizer(description_final,                                   \n",
    "                           truncation=True, \n",
    "                           max_length=MAXLEN, \n",
    "                           padding=\"max_length\")   \n",
    "        \n",
    "        input_ids = encodings_dict['input_ids']\n",
    "        attention_mask = encodings_dict['attention_mask']\n",
    "        \n",
    "        return {'label': torch.tensor(input_ids),\n",
    "                'input_ids': torch.tensor(input_ids), \n",
    "                'attention_mask': torch.tensor(attention_mask)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e8398807",
   "metadata": {},
   "outputs": [],
   "source": [
    "Train_dataset = Generate_dataset(X_train, tokenizer)\n",
    "Test_dataset = Generate_dataset(X_test, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bbbc6cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"GPT_models/\",\n",
    "    num_train_epochs=EPOCHS,\n",
    "    per_device_train_batch_size=TRAIN_BATCHSIZE,\n",
    "    per_device_eval_batch_size=TRAIN_BATCHSIZE,\n",
    "    gradient_accumulation_steps=BATCH_UPDATE,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy = \"epoch\",\n",
    "    fp16=True,\n",
    "    fp16_opt_level=APEX_OPT_LEVEL,\n",
    "    warmup_steps=WARMUP_STEPS,    \n",
    "    learning_rate=LR,\n",
    "    adam_epsilon=EPS,\n",
    "    weight_decay=0.01,        \n",
    "    save_total_limit=1,\n",
    "    load_best_model_at_end=True,     \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0a1e1be8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using amp fp16 backend\n",
      "***** Running training *****\n",
      "  Num examples = 9218\n",
      "  Num Epochs = 4\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "  Gradient Accumulation steps = 16\n",
      "  Total optimization steps = 576\n",
      "F:\\conda\\lib\\site-packages\\transformers\\trainer.py:1312: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='576' max='576' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [576/576 31:45, Epoch 3/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.912832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.845357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.820636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.736200</td>\n",
       "      <td>0.814085</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 1025\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to GPT_models/checkpoint-144\n",
      "Configuration saved in GPT_models/checkpoint-144\\config.json\n",
      "Model weights saved in GPT_models/checkpoint-144\\pytorch_model.bin\n",
      "tokenizer config file saved in GPT_models/checkpoint-144\\tokenizer_config.json\n",
      "Special tokens file saved in GPT_models/checkpoint-144\\special_tokens_map.json\n",
      "Deleting older checkpoint [GPT_models\\checkpoint-432] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1025\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to GPT_models/checkpoint-288\n",
      "Configuration saved in GPT_models/checkpoint-288\\config.json\n",
      "Model weights saved in GPT_models/checkpoint-288\\pytorch_model.bin\n",
      "tokenizer config file saved in GPT_models/checkpoint-288\\tokenizer_config.json\n",
      "Special tokens file saved in GPT_models/checkpoint-288\\special_tokens_map.json\n",
      "Deleting older checkpoint [GPT_models\\checkpoint-576] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1025\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to GPT_models/checkpoint-432\n",
      "Configuration saved in GPT_models/checkpoint-432\\config.json\n",
      "Model weights saved in GPT_models/checkpoint-432\\pytorch_model.bin\n",
      "tokenizer config file saved in GPT_models/checkpoint-432\\tokenizer_config.json\n",
      "Special tokens file saved in GPT_models/checkpoint-432\\special_tokens_map.json\n",
      "Deleting older checkpoint [GPT_models\\checkpoint-144] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1025\n",
      "  Batch size = 4\n",
      "Saving model checkpoint to GPT_models/checkpoint-576\n",
      "Configuration saved in GPT_models/checkpoint-576\\config.json\n",
      "Model weights saved in GPT_models/checkpoint-576\\pytorch_model.bin\n",
      "tokenizer config file saved in GPT_models/checkpoint-576\\tokenizer_config.json\n",
      "Special tokens file saved in GPT_models/checkpoint-576\\special_tokens_map.json\n",
      "Deleting older checkpoint [GPT_models\\checkpoint-288] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from GPT_models/checkpoint-576 (score: 0.8140851259231567).\n",
      "Saving model checkpoint to GPT_models/\n",
      "Configuration saved in GPT_models/config.json\n",
      "Model weights saved in GPT_models/pytorch_model.bin\n",
      "tokenizer config file saved in GPT_models/tokenizer_config.json\n",
      "Special tokens file saved in GPT_models/special_tokens_map.json\n"
     ]
    }
   ],
   "source": [
    "#---------------------------------------------------#\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,    \n",
    "    train_dataset=Train_dataset,\n",
    "    eval_dataset=Test_dataset,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "#---------------------------------------------------#\n",
    "trainer.train()\n",
    "trainer.save_model()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2bdd471d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "loading configuration file https://huggingface.co/gpt2/resolve/main/config.json from cache at C:\\Users\\yli62/.cache\\huggingface\\transformers\\fc674cd6907b4c9e933cb42d67662436b89fa9540a1f40d7c919d0109289ad01.7d2e0efa5ca20cef4fb199382111e9d3ad96fd77b849e1d4bed13a66e1336f51\n",
      "Model config GPT2Config {\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.10.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/gpt2/resolve/main/vocab.json from cache at C:\\Users\\yli62/.cache\\huggingface\\transformers\\684fe667923972fb57f6b4dcb61a3c92763ad89882f3da5da9866baf14f2d60f.c7ed1f96aac49e745788faa77ba0a26a392643a50bb388b9c04ff469e555241f\n",
      "loading file https://huggingface.co/gpt2/resolve/main/merges.txt from cache at C:\\Users\\yli62/.cache\\huggingface\\transformers\\c0c761a63004025aeadd530c4c27b860ec4ecbe8a00531233de21d865a402598.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
      "loading file https://huggingface.co/gpt2/resolve/main/tokenizer.json from cache at C:\\Users\\yli62/.cache\\huggingface\\transformers\\16a2f78023c8dc511294f0c97b5e10fde3ef9889ad6d11ffaa2a00714e73926e.cf2d0ecb83b6df91b3dbb53f1d1e4c311578bfd3aa0e04934215a49bf9898df0\n",
      "loading file https://huggingface.co/gpt2/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/gpt2/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/gpt2/resolve/main/tokenizer_config.json from cache at None\n",
      "loading configuration file https://huggingface.co/gpt2/resolve/main/config.json from cache at C:\\Users\\yli62/.cache\\huggingface\\transformers\\fc674cd6907b4c9e933cb42d67662436b89fa9540a1f40d7c919d0109289ad01.7d2e0efa5ca20cef4fb199382111e9d3ad96fd77b849e1d4bed13a66e1336f51\n",
      "Model config GPT2Config {\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.10.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "Assigning <|BOS|> to the bos_token key of the tokenizer\n",
      "Assigning <|EOS|> to the eos_token key of the tokenizer\n",
      "Assigning <|UNK|> to the unk_token key of the tokenizer\n",
      "Assigning <|PAD|> to the pad_token key of the tokenizer\n",
      "Assigning <|SEP|> to the sep_token key of the tokenizer\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Special tokens added\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/gpt2/resolve/main/config.json from cache at C:\\Users\\yli62/.cache\\huggingface\\transformers\\fc674cd6907b4c9e933cb42d67662436b89fa9540a1f40d7c919d0109289ad01.7d2e0efa5ca20cef4fb199382111e9d3ad96fd77b849e1d4bed13a66e1336f51\n",
      "Model config GPT2Config {\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50257,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50258,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"pad_token_id\": 50260,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"sep_token_id\": 50261,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.10.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/gpt2/resolve/main/pytorch_model.bin from cache at C:\\Users\\yli62/.cache\\huggingface\\transformers\\752929ace039baa8ef70fe21cdf9ab9445773d20e733cf693d667982e210837e.323c769945a351daa25546176f8208b3004b6f563438a7603e7932bae9025925\n",
      "All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
      "\n",
      "All the weights of GPT2LMHeadModel were initialized from the model checkpoint at gpt2.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = get_tokenier(special_tokens=SPECIAL_TOKENS)\n",
    "model = get_model(tokenizer, \n",
    "                  special_tokens=SPECIAL_TOKENS,\n",
    "                  load_model_path='GPT_models/pytorch_model.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "86ae5428",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if torch.cuda.is_available():\n",
    "#     device = torch.device('cuda:0')\n",
    "#     torch.backends.cudnn.deterministic = True\n",
    "#     torch.backends.cudnn.benchmark = False\n",
    "# else:\n",
    "device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f50d48ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50262, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50262, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1c399080",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 500/500 [21:46<00:00,  2.61s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "title = 'superior apartment with upscale customization services at commerce center area'\n",
    "prompt = SPECIAL_TOKENS['bos_token'] + title +SPECIAL_TOKENS['sep_token']\n",
    "\n",
    "generated = torch.tensor(tokenizer.encode(prompt)).unsqueeze(0)\n",
    "generated = generated.to(device)\n",
    "\n",
    "generated_sentences = []\n",
    "\n",
    "\n",
    "for _ in tqdm(range(500)):\n",
    "    sample_outputs = model.generate(generated, \n",
    "                                    do_sample=True,   \n",
    "                                    min_length=50, \n",
    "                                    max_length=MAXLEN,\n",
    "                                    top_k=30,                                 \n",
    "                                    top_p=0.7,        \n",
    "                                    temperature=0.9,\n",
    "                                    repetition_penalty=2.0,\n",
    "                                    num_return_sequences=1\n",
    "                                    )\n",
    "\n",
    "    for i, sample_output in enumerate(sample_outputs):\n",
    "        text = tokenizer.decode(sample_output, skip_special_tokens=True)\n",
    "        generated_sentences.append(text[len(title):])\n",
    "#     a = len(title) + len(','.join(keywords))    \n",
    "#     print(\"{}: {}\\n\\n\".format(i+1,  text[a:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "73966ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('good_sentence.txt','w') as f:\n",
    "    for line in generated_sentences:\n",
    "        f.write(line+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0ba77756",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 500/500 [19:10<00:00,  2.30s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "title = 'luxury villa with large private yard'\n",
    "prompt = SPECIAL_TOKENS['bos_token'] + title +SPECIAL_TOKENS['sep_token']\n",
    "\n",
    "generated = torch.tensor(tokenizer.encode(prompt)).unsqueeze(0)\n",
    "generated = generated.to(device)\n",
    "\n",
    "generated_sentences = []\n",
    "\n",
    "\n",
    "for _ in tqdm(range(500)):\n",
    "    sample_outputs = model.generate(generated, \n",
    "                                    do_sample=True,   \n",
    "                                    min_length=50, \n",
    "                                    max_length=MAXLEN,\n",
    "                                    top_k=30,                                 \n",
    "                                    top_p=0.7,        \n",
    "                                    temperature=0.9,\n",
    "                                    repetition_penalty=2.0,\n",
    "                                    num_return_sequences=1\n",
    "                                    )\n",
    "\n",
    "    for i, sample_output in enumerate(sample_outputs):\n",
    "        text = tokenizer.decode(sample_output, skip_special_tokens=True)\n",
    "        generated_sentences.append(text[len(title):])\n",
    "#     a = len(title) + len(','.join(keywords))    \n",
    "#     print(\"{}: {}\\n\\n\".format(i+1,  text[a:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3a2b2c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('exce_sentence.txt','w') as f:\n",
    "    for line in generated_sentences:\n",
    "        f.write(line+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f5a64a3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
